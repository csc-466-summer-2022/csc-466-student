{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6\n",
    "\n",
    "## Choosing among parameters when clustering\n",
    "\n",
    "### At the end of this lab, I should be able to\n",
    "* Formulate your own clustering questions and understand how you can go about getting answers\n",
    "* Understand how to select a clustering algorithm for your task\n",
    "\n",
    "**Note:** Exercises can be autograded and count towards your lab and assignment score. Problems are graded for participation.\n",
    "\n",
    "## Video Overview\n",
    "https://calpoly.zoom.us/rec/share/4GVIdxKEzA-af-wakibjPf8A27PSuWRMyCqehVx_fIhgj3wSt4CVJAq8z5KD8rT4.qy7mebr-OhR2-Vc9?startTime=1646517020000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "home = str(Path.home()) # all other paths are relative to this path. change to something else if this is not the case on your system\n",
    "REPO = f\"{home}/csc-466-student\"\n",
    "LAB = \"Lab6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### NO NEED TO EDIT ####\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from importlib import import_module\n",
    "helper = import_module(f'{LAB}_helper')\n",
    "#### NO NEED TO EDIT ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data\n",
    "We will be using a well known housing dataset from Boston.\n",
    "<pre>\n",
    "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
    " prices and the demand for clean air', J. Environ. Economics & Management,\n",
    " vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
    " ...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
    " pages 244-261 of the latter.\n",
    "\n",
    " Variables in order:\n",
    " CRIM     per capita crime rate by town\n",
    " ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    " INDUS    proportion of non-retail business acres per town\n",
    " CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    " NOX      nitric oxides concentration (parts per 10 million)\n",
    " RM       average number of rooms per dwelling\n",
    " AGE      proportion of owner-occupied units built prior to 1940\n",
    " DIS      weighted distances to five Boston employment centres\n",
    " RAD      index of accessibility to radial highways\n",
    " TAX      full-value property-tax rate per $10,000\n",
    " PTRATIO  pupil-teacher ratio by town\n",
    " B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    " LSTAT    % lower status of the population\n",
    " MEDV     Median value of owner-occupied homes in $1000's\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f\"{REPO}/data/housing/boston_fixed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1.** Read the descriptions of the features above, and come up with 2-3 reasonable questions with corresponding methods to test them. The only one that you cannot write, is the one I write below:\n",
    "\n",
    "Example questions: \n",
    "* Are there any definitive subgroupings (i.e., clusters) of towns in the dataset? \n",
    "* How many (if any) groups/clusters are there in the dataset?\n",
    "* Are there any clusters of median value of owner-occupied homes? And if so, can we use the rest of the data to predict these clusters? \n",
    "\n",
    "Methodology:\n",
    "1. Empirically determine the best clustering method from our known list of kmeans and hiearchical clustering.\n",
    "2. Using this best clustering model, visualize the data using PCA\n",
    "3. Apply clustering algorithms to MEDV and then use random forest to predict these clusters presenting the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload your solution to Canvas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall question: Are there any clusters of towns? \n",
    "\n",
    "Use the following methodology:\n",
    "\n",
    "1. Empirically determine the best clustering method from our known list kmeans and hiearchical clustering\n",
    "2. Using this best clustering, visualize the data using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** A lot of methods depend on the scaling of data, so we need to decide on a scaling method. We will use the autoscaling method described in sklearn as:\n",
    "\"The standard score of a sample x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "\n",
    "where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\" - <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">Source</a>\n",
    "\n",
    "For this exercise, scale ``df`` using the StandardScaler in sklearn. For consistency with later code, call this new scaled dataframe ``X``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = helper.scale(df)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{LAB}.py::test_exercise_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** We now need to take a look at our data, but it is too many dimensions! For this task we need to reduce the dimension. Reduce the dataset down to two dimensions using PCA. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">Here is a link to the documentation.</a> Store the transformed data in a variable called ``X_pca``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_pca = helper.pca(X)\n",
    "display(X_pca)\n",
    "X_pca.plot.scatter(x=X_pca.columns[0],y=X_pca.columns[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{LAB}.py::test_exercise_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** Our next major step is to apply kmeans to our data ``X`` (do NOT cluster on ``X_pca``) for several different values of ``k``. We'll compare these results later. The documentation for kmeans is <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">here</a>. Fill in the loop that constructs the kmeans models for each of the values of ``k`` specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmeans_models = helper.kmeans(X,range_n_clusters = [2, 3, 4, 5, 6],random_state=10)\n",
    "kmeans_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{LAB}.py::test_exercise_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** Now we need assign cluster labels to each sample in our dataset. Fill in the following to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_labels = helper.assign_labels(X,kmeans_models)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{LAB}.py::test_exercise_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 5 different clusterings of our data. We need to know which one of these is the best. Let's visualize the clusters (k=2 and k=3) using the cluster_labels and PCA. <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html\">Here is some documentation on how to set the color.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colorings = {}\n",
    "colorings[2] = cluster_labels[2].map({0: \"Blue\", 1: \"Red\"}) # This is a new pandas command for us that maps all 0 values to Blue, etc\n",
    "colorings[3] = cluster_labels[3].map({0: \"Blue\", 1: \"Red\",2: \"Pink\"}) # This is a new pandas command for us that maps all 0 values to Blue, etc\n",
    "X_pca.plot.scatter(x=X_pca.columns[0],y=X_pca.columns[1],c=colorings[2])\n",
    "X_pca.plot.scatter(x=X_pca.columns[0],y=X_pca.columns[1],c=colorings[3])\n",
    "colorings = {}\n",
    "colorings[2] = cluster_labels[2].map({0: \"Blue\", 1: \"Red\"}) # This is a new pandas command for us that maps all 0 values to Blue, etc\n",
    "colorings[3] = cluster_labels[3].map({0: \"Blue\", 1: \"Red\",2: \"Pink\"}) # This is a new pandas command for us that maps all 0 values to Blue, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a $k$\n",
    "We will now start assembling information we need to make a decision. There are many ways to evaluate clusters, but one of the best ways is through a silhouette score. Here is an excerpt of the documentation from sklearn:\n",
    "\"Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
    "\n",
    "Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\" - <a href=\"https://scikit-learn.org/dev/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#example-cluster-plot-kmeans-silhouette-analysis-py\">Source</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The silhouette_score gives the average value for all the samples.\n",
    "# This gives a perspective into the density and separation of the formed\n",
    "# clusters\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "n_clusters = 2\n",
    "silhouette_avg = silhouette_score(X, cluster_labels[n_clusters])\n",
    "print(\"For n_clusters =\", n_clusters,\n",
    "      \"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is pulled directly from https://en.wikipedia.org/wiki/Silhouette_(clustering).\n",
    "\n",
    "For data point $i\\in C_{i}$ (data point $i$ in the cluster $C_{i}$), let\n",
    "\n",
    "${\\displaystyle a(i)={\\frac {1}{|C_{i}|-1}}\\sum _{j\\in C_{i},i\\neq j}d(i,j)}$\n",
    "\n",
    "be the mean distance between ${\\displaystyle i}$ and all other data points in the same cluster, where ${\\displaystyle d(i,j)}$ is the distance between data points ${\\displaystyle i}$ and ${\\displaystyle j}$ in the cluster ${\\displaystyle C_{i}}$ (we divide by ${\\displaystyle |C_{i}|-1}$ because we do not include the distance ${\\displaystyle d(i,i)}$ in the sum). We can interpret ${\\displaystyle a(i)}$ as a measure of how well ${\\displaystyle i}$ is assigned to its cluster (the smaller the value, the better the assignment).\n",
    "\n",
    "We then define the mean dissimilarity of point ${\\displaystyle i}$ to some cluster ${\\displaystyle C_{k}}$ as the mean of the distance from ${\\displaystyle i}$ to all points in ${\\displaystyle C_{k}}$ (where ${\\displaystyle C_{k}\\neq C_{i}}$).\n",
    "\n",
    "For each data point ${\\displaystyle i\\in C_{i}}$, we now define\n",
    "\n",
    "${\\displaystyle b(i)=\\min _{k\\neq i}{\\frac {1}{|C_{k}|}}\\sum _{j\\in C_{k}}d(i,j)}$\n",
    "\n",
    "to be the smallest (hence the ${\\displaystyle \\min }$  operator in the formula) mean distance of ${\\displaystyle i}$ to all points in any other cluster, of which ${\\displaystyle i}$ is not a member. The cluster with this smallest mean dissimilarity is said to be the \"neighboring cluster\" of ${\\displaystyle i}$ because it is the next best fit cluster for point ${\\displaystyle i}$.\n",
    "\n",
    "We now define a silhouette (value) of one data point ${\\displaystyle i}$\n",
    "\n",
    "${\\displaystyle s(i)={\\frac {b(i)-a(i)}{\\max\\{a(i),b(i)\\}}}}$, if ${\\displaystyle |C_{i}|>1}$\n",
    "and ${\\displaystyle s(i)=0}$, if ${\\displaystyle |C_{i}|=1}$\n",
    "\n",
    "Which can be also written as:\n",
    "\n",
    "${\\displaystyle s(i)={\\begin{cases}1-a(i)/b(i),&{\\mbox{if }}a(i)<b(i)\\\\0,&{\\mbox{if }}a(i)=b(i)\\\\b(i)/a(i)-1,&{\\mbox{if }}a(i)>b(i)\\\\\\end{cases}}}$\n",
    "From the above definition it is clear that\n",
    "\n",
    "${\\displaystyle -1\\leq s(i)\\leq 1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5** Write your own silhouette_scores function that returns $s(i)$ for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = helper.silhouette_scores(X,cluster_labels[n_clusters])\n",
    "scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(scores) # do you match the sklearn implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{LAB}.py::test_exercise_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our plots\n",
    "Let's put it all together and grab the scores for each cluster. I'll take over the plotting here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s_df = pd.DataFrame(index=X.index,columns=cluster_labels.columns)\n",
    "for k in s_df.columns:\n",
    "    s_df.loc[:,k] = helper.silhouette_scores(X,cluster_labels[k])\n",
    "s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s_df.index.name=\"i\"\n",
    "s_df = s_df.reset_index()\n",
    "s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source = s_df.melt(id_vars=[\"i\"])\n",
    "source.columns = [\"i\",\"k\",\"s\"]\n",
    "\n",
    "import altair as alt\n",
    "alt.renderers.enable('mimetype')\n",
    "alt.Chart(source).mark_bar().encode(\n",
    "    x = \"s:Q\",\n",
    "    y = alt.Y(\"i:N\",sort='x',axis=alt.Axis(labels=False)),\n",
    "    row = \"k:N\",\n",
    "    color = \"k:N\"\n",
    ").resolve_scale(y='independent').properties(height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2: What are the average silhouttee scores for each value of $k$? Can you relate this average value to what you are seeing in the above plot? What kind of shape are we looking for?\n",
    "\n",
    "**Upload your solution to Canvas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiearchical Clustering\n",
    "\n",
    "From here on out there are several problems and only one exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3:** That was kmeans clustering. What about hiearchical clustering? For this excercise, use the same ``X`` data and create a dendrogram using hiearchical clustering. <a href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\">Here is a link to a sample</a>. After you dig into this code, answer what kind of linkage method was used (answer with more than just the name)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0,n_clusters=None)\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode='level', p=2)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload your solution to Canvas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4** Now change the linkage method to single linkage, and compare the plots. Are they better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload your solution to Canvas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering a single column to produce buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to switch gears and cluster the ``MEDV`` column. First, we will create a density plot of ``MEDV``. Make sure you go back to the original dataframe ``df`` at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = df[\"MEDV\"].plot.density();\n",
    "ax.set_xlabel('MEDV');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6** To me it looks reasonable that there might be 3 clusters as we have the shoulder sticking out around 30 and the bump at around 50. Using kmeans and k=3, group each town in one of three clusters using the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusterer = helper.bin_x(df[[\"MEDV\"]])\n",
    "labels = clusterer.predict(df[[\"MEDV\"]])\n",
    "df[\"y\"] = labels\n",
    "display(df)\n",
    "df.groupby(\"y\").MEDV.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{LAB}.py::test_exercise_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Good job!\n",
    "# Don't forget to push with ./submit.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having trouble with the test cases and the autograder?\n",
    "\n",
    "You can always load up the answers for the autograder. The autograder runs your code and compares your answer to the expected answer. I manually review your code, so there is no need to hide this from you.\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "answers = joblib.load(f\"{home}/csc-466-student/tests/answers_Lab6.joblib\")\n",
    "answers.keys()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
